{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Projeto 02\n",
    "\n",
    "Aprendizado Não-Supervisionado - Professor Mateus Mendelson\n",
    "\n",
    "Integrantes: Nasser Santiago Boan, Emmanuel Moreira, Harlan Martins\n",
    "\n",
    "___\n",
    "\n",
    "O objetivo desse projeto é implementar um algoritmo KMeans. O KMeans é um algoritmo de clusterização de aprendizado não supervisionado, muito utilizado para procurar subsets dentro um conjunto de dados que se assemelham em termos de suas características X.\n",
    "\n",
    "## Preparando os dados\n",
    "\n",
    "Para que os dados estejam prontos para ser usados devemos, inicialmente, executar o script 'make_dataset.py'. Conforme abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Lendo e tratando dataset!\n",
      ">> Dataset criado em 'data/meu_dataset.csv'\n",
      ">> O Dataset possui 1570 linhas e 3 colunas.\n"
     ]
    }
   ],
   "source": [
    "!python src/make_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o dataset carregado e criado, podemos agora começar a implementação do algoritmo. O algoritmo será implementado com o nome de classe MyKMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-8d64eb355fc5>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-8d64eb355fc5>\"\u001b[1;36m, line \u001b[1;32m68\u001b[0m\n\u001b[1;33m    for point in self.data: #df[['cases','deaths']].values\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "#alpha: indica a quantidade de tentativas sem melhoria de custo total médio que\n",
    "#devem ser realizadas antes da interrupção do algoritmo\n",
    "class MyPAM:\n",
    "    def __init__(self, k=3, max_iter=300, alpha=20, data=None):\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.data = data\n",
    "        self.medoids = []\n",
    "        self.ex_medoids = []\n",
    "        self.curr_error = np.max(self.data)\n",
    "    \n",
    "    \n",
    "    def random_medoid(self):\n",
    "        point = self.data[np.random.randint((len(self.data)))]\n",
    "        # nao podemos repetir medoids\n",
    "        while (point in self.ex_medoids):\n",
    "            point = self.data[np.random.randint((len(self.data)))]\n",
    "        return point\n",
    "    \n",
    "    def _initialize_medoids(self):\n",
    "        \n",
    "        initial_medoids = []\n",
    "        aux_index = []\n",
    "        \n",
    "        # garantir k indices diferentes\n",
    "        while (len(set(aux_index)) < self.k):\n",
    "            aux_index = []\n",
    "            for i in range(self.k):\n",
    "                aux_index.append(np.random.randint((len(self.data))))\n",
    "                            \n",
    "        for i in aux_index:\n",
    "            initial_medoids.append([self.data[i,0], self.data[i,1]])\n",
    "        \n",
    "        return np.array(initial_medoids)\n",
    "    \n",
    "    \n",
    "    def _medoid_to_substitute(self, new_medoid):\n",
    "        aux_index = 0\n",
    "        d = distance.euclidean(self.medoids[0], new_medoid)\n",
    "        for i in range(len(self.medoids)):\n",
    "            if (distance.euclidean(self.medoids[i], new_medoid) < d):\n",
    "                d = distance.euclidean(self.medoids[i], new_medoid)\n",
    "                aux_index = i\n",
    "        return i    \n",
    "    \n",
    "    # classification: dicionario contendo os conjuntos de pontos para cada cluster\n",
    "    def myMSE(self, classification):\n",
    "        s = []\n",
    "        for i in classification:\n",
    "            d = [\n",
    "                    (self.medoids[i][0] - p[0])**2 +\n",
    "                    (self.medoids[i][1] - p[1])**2 \n",
    "                    for p in classification[i]\n",
    "                ]\n",
    "            s.append(sum(d)/len(classification[i])) \n",
    "        \n",
    "        return sum(s)\n",
    "        \n",
    "    \n",
    "    def fit_pam(self,verbose=0):\n",
    "        self.medoids = self._initialize_medoids()\n",
    "        # os medoids não podem se repetir\n",
    "        self.ex_medoids = self.medoids\n",
    "        stop = self.medoids.copy()\n",
    "        count = 0\n",
    "        ## iterando sobre a quantidade max_iter\n",
    "        #for n_iter in range(self.max_iter): # = 300\n",
    "        iteracao = 0\n",
    "        tentativa_sem_melhora = 0\n",
    "        while (iteracao < self.max_iter) & (tentativa_sem_melhora < self.alpha):\n",
    "            \n",
    "            \n",
    "            ## criando os clusters\n",
    "            classification = {}\n",
    "            for cluster in range(self.k): #k = 3\n",
    "                classification[cluster] = []\n",
    "            ## calculando as distancias\n",
    "            for point in self.data: #df[['cases','deaths']].values\n",
    "                distancias = [math.sqrt((medoid[0] - point[0])**2 +\n",
    "                                        (medoid[1] - point[1])**2)\n",
    "                              for medoid in self.medoids]\n",
    "                ## achando o indice do menor valor na lista distancias\n",
    "                cluster = np.argmin(distancias)\n",
    "                ## adicionando esse ponto ao cluster \n",
    "                classification[cluster].append([value for value in point])\n",
    "                        \n",
    "            \n",
    "            \n",
    "            ## atribuindo novos medoids\n",
    "            \n",
    "            for i in range(len(self.medoids)):\n",
    "                new_medoid = self.random_medoid()\n",
    "                index = self._medoid_to_substitute(new_medoid)\n",
    "                self.ex_medoids += [self.medoids[index]]\n",
    "                self.medoids[index] = new_medoid\n",
    "            \n",
    "            \n",
    "            #for cluster in classification:\n",
    "            #    self.medoids[cluster] = np.mean(classification[cluster],axis=0)\n",
    "            \n",
    "            \n",
    "            ## implementando valor de tolerância\n",
    "            #if np.allclose(self.centroids, stop, atol=self.tol):\n",
    "            #    break\n",
    "            #stop = self.centroids.copy()\n",
    "            \n",
    "            \n",
    "            # verificar erro atual e se houve melhora\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ## contando iterações\n",
    "            iteracao+=1\n",
    "            #count += 1\n",
    "        \n",
    "        \n",
    "        if verbose == 1 :\n",
    "            print('Número de iterações: {}'.format(count))\n",
    "            \n",
    "        self.stop = stop\n",
    "        self.classification = classification\n",
    "        \n",
    "        \n",
    "    def show_plot(self):\n",
    "        \n",
    "        if self.k <= 10:\n",
    "        \n",
    "            data_framed = []\n",
    "\n",
    "            for cluster in self.classification:\n",
    "                for point in self.classification[cluster]:\n",
    "                    point.append(cluster)\n",
    "                    data_framed.append(point)\n",
    "\n",
    "            df_plot = pd.DataFrame(data_framed,columns=['cases','deaths','cluster'])\n",
    "            df_plot['cluster'] = df_plot['cluster'].astype('str')\n",
    "\n",
    "            fig = px.scatter(df_plot, x = 'cases', y = 'deaths', color = 'cluster', color_discrete_sequence=px.colors.qualitative.Plotly[:self.k],title='Data and Centroids')\n",
    "            fig.add_scatter(x = self.stop[:,0], y = self.stop[:,1],mode='markers',name='Centroid',marker=dict(size=[20 for _ in range(self.k)],symbol='x',color=px.colors.qualitative.Plotly[:self.k]))\n",
    "\n",
    "            fig.show()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('plot can only be constructed with 10 or less clusters.')\n",
    "            \n",
    "    def labels_(self):\n",
    "        \n",
    "        labels = []\n",
    "        \n",
    "        for cluster in range(self.k):\n",
    "            for i in range(len(self.classification[cluster])):\n",
    "                labels.append(cluster)\n",
    "        \n",
    "        return labels    \n",
    "    \n",
    "    def distances_(self,method):\n",
    "        \n",
    "        if method == 'mean':\n",
    "            \"\"\" O desafio pede a média de todas as distâncias a seus referentes clusters \"\"\"\n",
    "            dist = {}\n",
    "\n",
    "            for cluster in self.classification:\n",
    "                dist[cluster] = np.mean([math.sqrt((self.stop[cluster][0] - point[0])**2 + (self.stop[cluster][1] - point[1])**2) for point in self.classification[cluster]])\n",
    "                \n",
    "            return dist\n",
    "        \n",
    "        elif method == 'inertia':\n",
    "            \"\"\" Já inertia pede a soma \"\"\"\n",
    "            \n",
    "            dist = {}\n",
    "\n",
    "            for cluster in self.classification:\n",
    "                dist[cluster] = np.sum([((self.stop[cluster][0] - point[0])**2 + (self.stop[cluster][1] - point[1])**2) for point in self.classification[cluster]])\n",
    "                \n",
    "            inertia = sum(dist.values())\n",
    "                \n",
    "            return inertia\n",
    "        \n",
    "    def try_k(self,how_many=10):\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        self.original_k = self.k\n",
    "        \n",
    "        for k in range(2,how_many):\n",
    "            self.k = k\n",
    "            self.fit_k_means()\n",
    "            data.append(self.distances_('inertia'))\n",
    "        \n",
    "                \n",
    "        fig = go.Figure(data=go.Scatter(x = list(range(2,how_many)), y = data,marker_line_color=\"midnightblue\", marker_color=\"lightskyblue\", \n",
    "                           marker_line_width=2, marker_size=7))\n",
    "        fig.update_layout(title='Finding the Elbow Point')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o algortimo implementado acima podemos ler o dataset tratado e começar os testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## carregando o dataset tratado\n",
    "\n",
    "df = pd.read_csv('data/meu_dataset.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente vamos instanciar o algoritmo MyKMeans e guardar dentro da variável 'sol'. A construção da classe pede alguns parâmetros:\n",
    "- k --> A quantidade de clusters que iremos utilizar no algoritmo;\n",
    "- max_iter --> O número máximo de iterações que o algoritmo irá executar;\n",
    "- tol --> O valor de tolerância na distância entre o centroide do cluster de uma iteração e sua posição na iteração anterior.\n",
    "- data --> Dados para treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instanciando o algoritmo\n",
    "\n",
    "sol = MyPAM(k=5,data = df[['cases','deaths']].values,max_iter = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['cases','deaths']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente vamos testar vários valores de K para descobrirmos o valor ótimo através do método Elbow. Nesse caso testaremos nosso modelos num range de 2 até 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol.try_k(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No eixo Y temos a inércia de cada iteração do algoritmo. Percebemos que quando k assume o valor 4 temos um ponto de inflexão, ou seja, as inércias subsequentes não demonstram grande variação em seu valor comparando com o mesmo valor em k-1. Levando em consideração os resultados acima podemos treinar o algoritmo levando em consideração K = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reinstanciando o modelo com novo valor de K\n",
    "\n",
    "sol = MyPAM(k=4,data = df[['cases','deaths']].values,max_iter = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora estamos prontos para treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## treinando o modelo\n",
    "\n",
    "sol.fit_pam(verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao treinar o algoritmo ele diz quantas iterações foram necessárias para convergir levando em consideração a tolerâncio configurada em seu instanciamento. Podemos, então verificar os pontos e seus centróides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.show_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada cluster acima com seu centróide associado representam pontos, ou nesse caso cidades, que possuem um comportamento de casos e mortes de covid-19 semelhantes. Podemos analisar as médias das distâncias dos pontos de cada cluster a seu centróide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrando as distâncias\n",
    "\n",
    "sol.distances_('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretando as distâncias acima podemos averiguar que o cluster 3 é o mais assertivo, visto o valor zerado, na verdade podemos dizer, com certeza, que o centróide desse cluster está exatamente na mesma coordenada do seu único ponto. Em segundo lugar, temos o cluster 0 com uma distância média de 1414, ou seja, seus pontos estão próximos ao centróide. Em terceiro lugar temos o cluster 2 demosntrando uma distância média de 4243 de seus pontos e, em último lugar, temos o cluster 1 com uma distância de 15765 esse alto valor demonstra que o cluster possui pontos esparsos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferindo a inércia do algoritmo montado podemos ver que nossa versão está bem próxima da implementação do próprio KMeans do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inércia do MyKMeans\n",
    "\n",
    "sol.distances_('inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inércia do sklearn.cluster.KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "KMeans(n_clusters=4,max_iter=300,tol=0.0001).fit(df[['cases','deaths']].values).inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O único downside do algoritmo montado nesse jupyter notebook é em termos de desempenho. Conforme demonstrado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "KMeans(n_clusters=4,max_iter=300,tol=0.0001).fit(df[['cases','deaths']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sol = MyKMeans(k=4,max_iter=300,tol=0.0001,data=df[['cases','deaths']].values)\n",
    "sol.fit_k_means()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
